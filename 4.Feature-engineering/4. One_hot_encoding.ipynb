{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library and disable warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import OneHotEncoder for one hot encoding with sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding - \"Usable and Useful ML Product\"\n",
    "\n",
    "Just like imputation, all methods of categorical encoding should be performed over the training set, and then propagated to the test set. \n",
    "\n",
    "Why? \n",
    "\n",
    "Because these methods will \"learn\" patterns from the train data, and therefore you want to avoid leaking information and overfitting. But more importantly, because we don't know whether in future / live data, we will have all the categories present in the train data, or if there will be more or less categories. Therefore, we want to anticipate this uncertainty by setting the right processes right from the start. We want to create transformers that learn the categories from the train set, and used those learned categories to create the dummy variables in both train and test sets.\n",
    "\n",
    "In this notebook we'll be using Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset using columns 'Survived','Sex','Embarked','Cabin'\n",
    "data = pd.read_csv('Data/titanic_data.csv', usecols = ['Survived','Sex','Embarked','Cabin'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use str[] to capture only first letter of Cabin\n",
    "data['Cabin'] = data['Cabin'].str[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['Sex', 'Embarked','Cabin']],  \n",
    "    data['Survived'],  \n",
    "    test_size=0.3,  \n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore the cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of columns in data\n",
    "for col in list(data):\n",
    "    print(col)\n",
    "    print(data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. One-Hot Encoding with Pandas\n",
    "\n",
    "**Advantages:**\n",
    "- quick\n",
    "- returns Pandas DataFrame \n",
    "- returns feature names for the dummy variables\n",
    "- accepts missing values\n",
    "\n",
    "**Limitations:**\n",
    "- it does not preserve information from train data to propagate to test data\n",
    "\n",
    "-----\n",
    "\n",
    "The pandas method `get_dummies()`, will create as many binary variables as categories in the variable:\n",
    "\n",
    "If the variable colour has 3 categories in the train data, it will create 2 dummy variables. However, if the variable colour has 5 categories in the test data, it will create 4 binary variables, therefore train and test sets will end up with different number of features and will be incompatible with training and scoring using Scikit-learn.\n",
    "\n",
    "In practice, we shouldn't be using `get_dummies()` in our machine learning pipelines. It is however useful, for a quick data exploration. Let's look at this with examples.\n",
    "\n",
    "### into k-dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(X_train['Sex'])\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat original Pclass Series with created dummy variables for visualization what happend\n",
    "result = pd.concat([X_train['Sex'], pd.get_dummies(X_train['Sex'])], axis = 1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 >>>> Get dummy variables for column 'Embarked'\n",
    "#             Concat original 'Embarked' Series and store it in variable result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get dummy variable for all variables at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variable for entire train set\n",
    "dummy_data = pd.get_dummies(X_train)\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>> Get dummy variable for entire test set\n",
    "dummy = pd.get_dummies(X_test)\n",
    "dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrames have features names what is the advantage of Pandas `get_dummies()`. On the other hand there is an issue and so train set contain more features than test set. The reason of this is that test set does not contain feature 'Cabin_T', therefore train and test sets do not have the same shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding with Scikit-learn\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- quick\n",
    "- creates the same number of features in train and test set\n",
    "- by default, the encoder derives the categories based on the unique values in each feature\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- it returns a numpy array instead of a pandas dataframe if we do not specify otherwise\n",
    "- it does not return the variable names, therefore inconvenient for variable exploration\n",
    "- it does not except missing values (Pandas `.get_dummies()` does)\n",
    "\n",
    "You can find more information about One-Hot Encoder [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Encoder\n",
    "# Set parameter sparse = False to return sparse matrix\n",
    "# Set parameter handle_unknown = 'error' to raise an error if an unknown categorical feature is present during transform\n",
    "encoder = OneHotEncoder(categories = 'auto', sparse = False, handle_unknown = 'error')\n",
    "\n",
    "#  Fit the Encoder and fill in missing values with method ffill\n",
    "encoder.fit(X_train.fillna('Missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get categories with the .categories_ attribute\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using one-hot encoding\n",
    "training_set = encoder.transform(X_train.fillna('Missing'))\n",
    "pd.DataFrame(training_set).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after transforming of the data, the features names are not returned. We can retrieve these features names using `.get_feature_names()`, we'll repeat the entire process of transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using one-hot encoding and return feature names for output features \n",
    "# Convert it to DataFrame\n",
    "training_set = encoder.transform(X_train.fillna('Missing'))\n",
    "training_set = pd.DataFrame(training_set)\n",
    "training_set.columns = encoder.get_feature_names()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>>>>>>\n",
    "# Transform X_test using one-hot encoding and return feature names for output features, store it to variable testing_set\n",
    "# Convert it to DataFrame\n",
    "# Inspect the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training set and testing set contain the same number of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
